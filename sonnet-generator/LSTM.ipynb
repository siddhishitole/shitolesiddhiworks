{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from file and split into characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(filename):\n",
    "    chars = set()\n",
    "\n",
    "    text = ''\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            if len(words) <= 1:\n",
    "                continue\n",
    "\n",
    "            line = line.lower()\n",
    "            chars |= set(line)\n",
    "\n",
    "            text += line\n",
    "            \n",
    "    return chars, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoding(chars):\n",
    "    id_to_char = list(chars)\n",
    "    char_to_ids = {}\n",
    "    for i, c in enumerate(id_to_char):\n",
    "        char_to_ids[c] = i\n",
    "        \n",
    "    return id_to_char, char_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(c, char_to_ids):\n",
    "    encoding = np.zeros((M, ))\n",
    "    encoding[char_to_ids[c]] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_string(s, char_to_ids):\n",
    "    encoded = np.zeros((len(s), len(char_to_ids)))\n",
    "    for i, c in enumerate(s):\n",
    "        encoded[i] = encode(c, char_to_ids)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning raw text into sequences of characters for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_sequences(char_to_ids, skip=3, seq_len=40):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(0, len(text) - seq_len - 1, skip):\n",
    "        sequence = []\n",
    "        for j in range(i, i + seq_len):\n",
    "            sequence.append(encode(text[j], char_to_ids))\n",
    "        X.append(sequence)\n",
    "        y.append(encode(text[i + seq_len], char_to_ids))\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94248, 40, 38) (94248, 38)\n"
     ]
    }
   ],
   "source": [
    "chars, text = get_data_from_file('data/shakespeare.txt')\n",
    "id_to_char, char_to_ids = build_encoding(chars)\n",
    "M = len(id_to_char)\n",
    "skip, seq_len = 1, 40\n",
    "\n",
    "X, y = get_encoded_sequences(char_to_ids, skip, seq_len)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, input_shape=X.shape[1:]))\n",
    "    model.add(Dense(M, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM = LSTM_model()\n",
    "modelLSTM.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "94248/94248 [==============================] - 87s 923us/step - loss: 2.3143\n",
      "Epoch 2/50\n",
      "94248/94248 [==============================] - 85s 907us/step - loss: 1.9031\n",
      "Epoch 3/50\n",
      "94248/94248 [==============================] - 83s 881us/step - loss: 1.7541\n",
      "Epoch 4/50\n",
      "94248/94248 [==============================] - 82s 874us/step - loss: 1.6582\n",
      "Epoch 5/50\n",
      "94248/94248 [==============================] - 81s 861us/step - loss: 1.5819\n",
      "Epoch 6/50\n",
      "94248/94248 [==============================] - 80s 850us/step - loss: 1.5173\n",
      "Epoch 7/50\n",
      "94248/94248 [==============================] - 82s 874us/step - loss: 1.4597\n",
      "Epoch 8/50\n",
      "94248/94248 [==============================] - 82s 868us/step - loss: 1.4019\n",
      "Epoch 9/50\n",
      "94248/94248 [==============================] - 81s 862us/step - loss: 1.3489\n",
      "Epoch 10/50\n",
      "94248/94248 [==============================] - 81s 858us/step - loss: 1.2945\n",
      "Epoch 11/50\n",
      "94248/94248 [==============================] - 80s 852us/step - loss: 1.2437\n",
      "Epoch 12/50\n",
      "94248/94248 [==============================] - 80s 850us/step - loss: 1.1917\n",
      "Epoch 13/50\n",
      "94248/94248 [==============================] - 80s 851us/step - loss: 1.1420\n",
      "Epoch 14/50\n",
      "94248/94248 [==============================] - 80s 854us/step - loss: 1.0955\n",
      "Epoch 15/50\n",
      "94248/94248 [==============================] - 80s 852us/step - loss: 1.0518\n",
      "Epoch 16/50\n",
      "94248/94248 [==============================] - 80s 849us/step - loss: 1.0110\n",
      "Epoch 17/50\n",
      "94248/94248 [==============================] - 81s 859us/step - loss: 0.9732\n",
      "Epoch 18/50\n",
      "94248/94248 [==============================] - 83s 876us/step - loss: 0.9387\n",
      "Epoch 19/50\n",
      "94248/94248 [==============================] - 83s 882us/step - loss: 0.9074\n",
      "Epoch 20/50\n",
      "94248/94248 [==============================] - 81s 855us/step - loss: 0.8794\n",
      "Epoch 21/50\n",
      "94248/94248 [==============================] - 81s 864us/step - loss: 0.8507\n",
      "Epoch 22/50\n",
      "94248/94248 [==============================] - 80s 848us/step - loss: 0.8327\n",
      "Epoch 23/50\n",
      "94248/94248 [==============================] - 80s 851us/step - loss: 0.8081\n",
      "Epoch 24/50\n",
      "94248/94248 [==============================] - 80s 852us/step - loss: 0.7872\n",
      "Epoch 25/50\n",
      "94248/94248 [==============================] - 81s 859us/step - loss: 0.7690\n",
      "Epoch 26/50\n",
      "94248/94248 [==============================] - 80s 848us/step - loss: 0.7549\n",
      "Epoch 27/50\n",
      "94248/94248 [==============================] - 81s 864us/step - loss: 0.7360\n",
      "Epoch 28/50\n",
      "94248/94248 [==============================] - 82s 868us/step - loss: 0.7205\n",
      "Epoch 29/50\n",
      "94248/94248 [==============================] - 79s 842us/step - loss: 0.7096\n",
      "Epoch 30/50\n",
      "94248/94248 [==============================] - 79s 842us/step - loss: 0.6966\n",
      "Epoch 31/50\n",
      "94248/94248 [==============================] - 81s 861us/step - loss: 0.6805\n",
      "Epoch 32/50\n",
      "94248/94248 [==============================] - 79s 841us/step - loss: 0.6698\n",
      "Epoch 33/50\n",
      "94248/94248 [==============================] - 79s 843us/step - loss: 0.6631\n",
      "Epoch 34/50\n",
      "94248/94248 [==============================] - 80s 844us/step - loss: 0.6537\n",
      "Epoch 35/50\n",
      "94248/94248 [==============================] - 79s 842us/step - loss: 0.6372\n",
      "Epoch 36/50\n",
      "94248/94248 [==============================] - 79s 839us/step - loss: 0.6312\n",
      "Epoch 37/50\n",
      "94248/94248 [==============================] - 79s 838us/step - loss: 0.6209\n",
      "Epoch 38/50\n",
      "94248/94248 [==============================] - 79s 840us/step - loss: 0.6165\n",
      "Epoch 39/50\n",
      "94248/94248 [==============================] - 79s 837us/step - loss: 0.6085\n",
      "Epoch 40/50\n",
      "94248/94248 [==============================] - 79s 837us/step - loss: 0.5975\n",
      "Epoch 41/50\n",
      "94248/94248 [==============================] - 79s 836us/step - loss: 0.5898\n",
      "Epoch 42/50\n",
      "94248/94248 [==============================] - 79s 834us/step - loss: 0.5871\n",
      "Epoch 43/50\n",
      "94248/94248 [==============================] - 79s 834us/step - loss: 0.5748\n",
      "Epoch 44/50\n",
      "94248/94248 [==============================] - 79s 835us/step - loss: 0.5711\n",
      "Epoch 45/50\n",
      "94248/94248 [==============================] - 79s 834us/step - loss: 0.5600\n",
      "Epoch 46/50\n",
      "94248/94248 [==============================] - 79s 833us/step - loss: 0.5580\n",
      "Epoch 47/50\n",
      "94248/94248 [==============================] - 79s 834us/step - loss: 0.5501\n",
      "Epoch 48/50\n",
      "94248/94248 [==============================] - 79s 840us/step - loss: 0.5453\n",
      "Epoch 49/50\n",
      "94248/94248 [==============================] - 79s 837us/step - loss: 0.5443\n",
      "Epoch 50/50\n",
      "94248/94248 [==============================] - 79s 840us/step - loss: 0.5315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25381073388>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLSTM.fit(X, y, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM.save('models/LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM2 = load_model('models/LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"shall i compare thee to a summer's day?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'r', 's', 'p', 'l', 'v', '.', 'z', '\\n', 'q', 'y', 'd', 'c', 'n', '?', 'k', 'o', 'g', 'h', ' ', \"'\", ';', 'j', 't', 'u', 'b', '-', 'w', 'x', '!', 'i', 'a', ',', ')', 'f', ':', '(', 'e']\n",
      "{'m': 0, 'r': 1, 's': 2, 'p': 3, 'l': 4, 'v': 5, '.': 6, 'z': 7, '\\n': 8, 'q': 9, 'y': 10, 'd': 11, 'c': 12, 'n': 13, '?': 14, 'k': 15, 'o': 16, 'g': 17, 'h': 18, ' ': 19, \"'\": 20, ';': 21, 'j': 22, 't': 23, 'u': 24, 'b': 25, '-': 26, 'w': 27, 'x': 28, '!': 29, 'i': 30, 'a': 31, ',': 32, ')': 33, 'f': 34, ':': 35, '(': 36, 'e': 37}\n"
     ]
    }
   ],
   "source": [
    "print(id_to_char)\n",
    "print(char_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_softmax(prediction, temp=1.0):\n",
    "    prediction = np.asarray(prediction).astype('float64')\n",
    "    num = np.log(prediction) / temp\n",
    "    num = np.exp(num)\n",
    "    p = num / np.sum(num)\n",
    "    return np.argmax(np.random.multinomial(1, p, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(num_chars, temp=1):\n",
    "    generated = seed\n",
    "    sequence = seed\n",
    "    for i in range(num_chars):\n",
    "        x = np.zeros((1, len(sequence), len(char_to_ids)))\n",
    "        x[0] = encode_string(sequence, char_to_ids)\n",
    "        prediction = modelLSTM2.predict(x)\n",
    "        next_char_id = sample_from_softmax(prediction[0], temp)\n",
    "        next_char = id_to_char[next_char_id]\n",
    "        generated += next_char\n",
    "        sequence = sequence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "this more so, to dreds doth cruel kind,\n",
      "and dountet baster sourly ougle worth\n",
      "than in his did falled with thine and true,\n",
      "to the dung shall in thy worth and sun,\n",
      "corrain to me, so thrue love we come woe,\n",
      "  to wint, of hove, which in shy hide is love,\n",
      "and thence this, and thou shouldst not to deam.\n",
      "that times his beauty do i quent straight,\n",
      "and therefore waste in thy cheaked are dead.\n",
      "simpot night by mine eye more dear.\n",
      "  as the prey in thee in dead do thee,\n",
      "  and this mine in of thee best thought cannotime,\n",
      "to let my barest which the time that pups\n",
      "crows of hearts have present upon thy name,\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "num_chars = 600\n",
    "generated = generate_sequence(num_chars, temp=0.25)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "th' expense of shapes disgrainiag side,\n",
      "  but heat the child, and nothing this edd\n",
      "that i am potson brongs thence boss,\n",
      "  whose some suspect of me forbor do thee,\n",
      "for bending foul and other ppainting\n",
      "earth, shall on me brain short-lie thee,\n",
      "where awas the true, supposed of woitetion,\n",
      "and somety seeming menours make be grief,\n",
      "they left me doth parts of outward dwast.\n",
      "when i wondrow that i sweet love his,\n",
      "and do i in hand of him in a don.\n",
      "than that wend'rn of mine own prese behings,\n",
      "and thou taughts their thy formen alenthered,\n",
      "alove's green appier thou shalt same life,\n",
      "and cheeks ne'er runting \n"
     ]
    }
   ],
   "source": [
    "num_chars = 600\n",
    "generated = generate_sequence(num_chars, temp=0.75)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "th' eabs targed fors-your veriow,\n",
      "if fortur breast doth rire poath of ever.\n",
      "weth.\n",
      "  my hours readons, where you freme with thy find,\n",
      "both looks, deiting morture amesty,\n",
      "but those wanting owerst love his, behble:\n",
      "thou hast parts to win, presence ffomous,\n",
      "o gover-knot her pyoures. shall by cart?\n",
      "now which my devise in huse it sond,\n",
      "i see burn of not, to lief the thing or ween,\n",
      "thy i'ts to tell not wifter-despite,\n",
      "cortine quite of secorain ow hate of minere,\n",
      "thou natured thus i tenture hatour ad.\n",
      "that meast that i have seem not love to to me.\n",
      "'has vasing with mire, i never thangeryparch?\n",
      "exceared\n"
     ]
    }
   ],
   "source": [
    "num_chars = 600\n",
    "generated = generate_sequence(num_chars, temp=1.5)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "37_env",
   "language": "python",
   "name": "37_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
