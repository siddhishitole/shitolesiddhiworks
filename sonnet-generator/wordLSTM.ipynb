{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Activation\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, preprocess the data the same way as for the HMM - get a list of words, and remove all punctuation and the start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "dictionary = set()\n",
    "with open('data/Syllable_dictionary.txt') as f:\n",
    "    for line in f:\n",
    "        word_list.append(line.split()[0])\n",
    "        dictionary.add(line.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    punctuation = [',','.',':','?',';','!',\"'\",'\"', '(', ')']\n",
    "    for i, word in enumerate(words):\n",
    "        word = word.lower()\n",
    "\n",
    "        while word not in dictionary:\n",
    "            \n",
    "            if word[-1] in punctuation:\n",
    "                word = word[:-1]\n",
    "\n",
    "            if word[0] in punctuation and word not in dictionary:\n",
    "                word = word[1:]\n",
    "\n",
    "        words[i] = word\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the raw data from the file into a list of words. Treat the newline character as its own word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        start = 0\n",
    "        for i, line in enumerate(f):\n",
    "            words = line.split()\n",
    "            if len(words) <= 1:\n",
    "                continue\n",
    "                \n",
    "            words = remove_punctuation(words)\n",
    "            words.append('\\n')\n",
    "            data.extend(words)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign each word a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data, word_list):\n",
    "    encoding = {}\n",
    "    for i, word in enumerate(word_list):\n",
    "        encoding[word] = i\n",
    "    \n",
    "    encoded_data = []\n",
    "    for word in data:\n",
    "        encoded_data.append(encoding[word])\n",
    "        \n",
    "    return encoded_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19737\n"
     ]
    }
   ],
   "source": [
    "word_data = get_words_from_data('data/shakespeare.txt')\n",
    "print(len(word_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list2 = ['\\n'] + word_list\n",
    "vocab_size = len(word_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data_encoded = encode_data(word_data, word_list2)\n",
    "word_data_encoded = np.array(word_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training data by taking sequences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_data(words, vocab_size, skip=3, seq_len=20):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(0, len(words) - seq_len - 1, skip):\n",
    "        sequence = words[i : i + seq_len]\n",
    "        X.append(sequence)\n",
    "        y.append(to_categorical(words[i + 1 : i + seq_len + 1], num_classes=vocab_size))\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19716, 20) (19716, 20, 3206)\n"
     ]
    }
   ],
   "source": [
    "skip = 1\n",
    "seq_len = 20\n",
    "X, y = build_training_data(word_data_encoded, vocab_size, skip, seq_len)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model2(seq_len=20):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 200, input_length=seq_len))\n",
    "    model.add(LSTM(200, input_shape=X.shape[1:], return_sequences=True))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 20, 200)           641200    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 20, 200)           320800    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20, 3206)          644406    \n",
      "=================================================================\n",
      "Total params: 1,606,406\n",
      "Trainable params: 1,606,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_model = LSTM_model2()\n",
    "embedding_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.9395\n",
      "Epoch 2/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.7799\n",
      "Epoch 3/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.6567\n",
      "Epoch 4/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.5626\n",
      "Epoch 5/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.4926\n",
      "Epoch 6/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.4402\n",
      "Epoch 7/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.4017\n",
      "Epoch 8/40\n",
      "19716/19716 [==============================] - 66s 3ms/step - loss: 0.3726\n",
      "Epoch 9/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.3507\n",
      "Epoch 10/40\n",
      "19716/19716 [==============================] - 66s 3ms/step - loss: 0.3333\n",
      "Epoch 11/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.3196\n",
      "Epoch 12/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.3085\n",
      "Epoch 13/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2989\n",
      "Epoch 14/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.2915\n",
      "Epoch 15/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2845\n",
      "Epoch 16/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2788\n",
      "Epoch 17/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.2738\n",
      "Epoch 18/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.2695\n",
      "Epoch 19/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.2658\n",
      "Epoch 20/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.2623\n",
      "Epoch 21/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2591\n",
      "Epoch 22/40\n",
      "19716/19716 [==============================] - 65s 3ms/step - loss: 0.2566\n",
      "Epoch 23/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2543\n",
      "Epoch 24/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2517\n",
      "Epoch 25/40\n",
      "19716/19716 [==============================] - 63s 3ms/step - loss: 0.2500\n",
      "Epoch 26/40\n",
      "19716/19716 [==============================] - 63s 3ms/step - loss: 0.2479\n",
      "Epoch 27/40\n",
      "19716/19716 [==============================] - 63s 3ms/step - loss: 0.2465\n",
      "Epoch 28/40\n",
      "19716/19716 [==============================] - 63s 3ms/step - loss: 0.2450\n",
      "Epoch 29/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2434\n",
      "Epoch 30/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2422\n",
      "Epoch 31/40\n",
      "19716/19716 [==============================] - 63s 3ms/step - loss: 0.2409\n",
      "Epoch 32/40\n",
      "19716/19716 [==============================] - 63s 3ms/step - loss: 0.2399\n",
      "Epoch 33/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2389\n",
      "Epoch 34/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2377\n",
      "Epoch 35/40\n",
      "19716/19716 [==============================] - 63s 3ms/step - loss: 0.2370\n",
      "Epoch 36/40\n",
      "19716/19716 [==============================] - 63s 3ms/step - loss: 0.2362\n",
      "Epoch 37/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2354\n",
      "Epoch 38/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2346\n",
      "Epoch 39/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2342\n",
      "Epoch 40/40\n",
      "19716/19716 [==============================] - 64s 3ms/step - loss: 0.2332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2266141dc08>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.fit(X, y, batch_size=32, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.save('wordLSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_softmax(prediction, temp=1.0):\n",
    "    prediction = np.asarray(prediction).astype('float64')\n",
    "    num = np.log(prediction) / temp\n",
    "    num = np.exp(num)\n",
    "    p = num / np.sum(num)\n",
    "    return np.argmax(np.random.multinomial(1, p, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(sequence, word_list):\n",
    "    decoded = \"\"\n",
    "    for word_id in sequence:\n",
    "        decoded += word_list[word_id]\n",
    "        if word_id != 0:\n",
    "            decoded += \" \"\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(num_words, temp=1):\n",
    "    generated = []\n",
    "    sequence = seed\n",
    "    for i in range(num_words):\n",
    "        x_pred = np.zeros((1, sequence.shape[0]))\n",
    "        x_pred[0] = sequence\n",
    "        prediction = embedding_model.predict(x_pred)\n",
    "        next_word_id = sample_from_softmax(prediction[0][-1], temp)\n",
    "        generated.append(next_word_id)\n",
    "        sequence[:-1] = sequence[1:]\n",
    "        sequence[-1] = next_word_id\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = \"shall i compare thee to a summer's day \\n thou art more lovely and more temperate \\n rough winds do\"\n",
    "initwords = init.split(\" \")\n",
    "seed = np.array(encode_data(initwords, word_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kehua\\AppData\\Local\\Programs\\Python\\python-venv\\37_env\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "generated = generate_sequence(150, temp=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day \n",
      " thou art more lovely and more temperate \n",
      " rough winds do a story of his spring \n",
      "for it depends upon that love doth part \n",
      "and summer's lease hath all too short a date \n",
      "sometime too hot the self who eyes have been on every vulgar thief \n",
      "thee have i not any thing sinful then striving to mend \n",
      "to mar the subject that before was any dear prepare her \n",
      "made old offences of affections new \n",
      "most true worse say thy beauty's form upon desired change \n",
      "o hear and dove it will \n",
      "so him i lose through my woeful age \n",
      "by this be error and upon thine own desire is admitted when i since others voices that my adder's sense \n",
      "to critic and to flatterer stopped are seen \n",
      "and heavily in eyes \n",
      "showing their garments though new-fangled no \n",
      "for at a death hath do that harvest reap \n",
      "at \n"
     ]
    }
   ],
   "source": [
    "print(init + \" \" + decode_sequence(generated, word_list2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "37_env",
   "language": "python",
   "name": "37_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
